seed: 42
num_proc: 64

# --- Stage 1: Initial, Fast Preprocessing ---
# Set 'load_stage_one_from_hub' to a Hugging Face dataset ID to skip Stage 1
# and download the intermediate data directly. Set to null to run Stage 1 locally.
load_stage_one_from_hub: "helena-balabin/vg-captions-graphs"

vg_metadata_dir: "/data/vg/vg_metadata"

vg_metadata_hf_identifier: "helena-balabin/vg-without-coco-captions"
vg_metadata_split: "train"
vg_captions_column: "caption"
vg_processed_dir: "/data/huggingface/datasets/vg-captions-graphs"
vg_coco_overlap_hf_identifier: "helena-balabin/vg_actions_spatial_for_graphormer_with_text"
vg_coco_split: "train"
vg_coco_column: "coco_id"
cache_dir: "/data/huggingface/datasets"

vg_image_id_col: "image_id"
include_image_graphs: True
flatten_captions: True

# --- Stage 2: Final, Heavy Preprocessing ---
# This stage loads images and runs the CLIP processor, creating the final training-ready dataset.

# Base path where the images are stored
image_base_path: "/data/huggingface/vg/VG_100K"

# Path to save the final, fully preprocessed dataset. !!! Use scratch for that !!!
# Could be around 700 GB for the three graph types.
preprocessed_output_path: "/data/huggingface/datasets/vg-captions-graphs-processed"
# TODO possibly add preprocessed split

# Batch size for the final mapping function
preprocessing_batch_size: 64

# Whether to overwrite the cache for the final, slow preprocessing step
overwrite_cache: false
max_shard_size: "8GB"

# Set to true to push the final dataset to the Hugging Face Hub
push_to_hub: true
hf_dataset_identifier_processed: "helena-balabin/vg-captions-graphs-processed"

# Model details needed for the CLIPProcessor
model:
  pretrained_model_name_or_path: "openai/clip-vit-base-patch32"
  cache_dir: "/data/huggingface/transformers/"
  graph_types:
    - "image_graphs"
    - "action_image_graphs"
    - "spatial_image_graphs"

# Training details needed for preprocess_item
training:
  edge_max_dist: 20