output_dir: /data/huggingface/transformers

data:
  dataloader_num_workers: 1
  dataset_identifier: "helena-balabin/vg-captions-graphs"
  cache_dir: /data/huggingface/datasets/
  seed: 42
  validation_split: 0.1
  max_edges: 40

model:
  huggingface_hub_model_id: "helena-balabin/graphormer-cl"
  cache_dir: /data/huggingface/transformers/
  graph_types: ["image", "spatial_image", "action_image"] # List of graph types to train models for
  graphormer_size: "small"
  dropout: 0.1

training:
  batch_size: 256 # 32 for debug, also 256 * 4 GPUs is effective batch size of 1024
  gradient_accumulation_steps: 1
  prefetch_factor: 2
  learning_rate: 3e-4
  max_steps: 2500 # 100 steps for debug
  weight_decay: 0.01  # or 0.01
  logging_steps: 100 # 10 for debug
  eval_steps: 250  # 10 for debug
  save_steps: 500
  save_total_limit: 3
  warmup_ratio: 0.05 # optional: set to 0.05
  precision: "bf16" # options: "bf16", "fp16", "fp32"
  pin_memory: true
  persistent_workers: true
  dataloader_drop_last: true
  lr_scheduler_type: "linear"  # options: "linear", "constant", "cosine", etc.

mlflow:
  tracking_uri: /data/mlflow/
  experiment_name: "Graphormer Contrastive Learning"
