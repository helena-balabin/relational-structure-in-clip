output_dir: /data/huggingface/transformers

data:
  dataloader_num_workers: 16
  local_dataset_identifier_processed: "/data/huggingface/datasets/vg-captions-graphs-processed/processed"
  seed: 42
  validation_split: 0.1
  remove_columns: ["coco_id", "flickr_id"]

model:
  pretrained_model_name_or_path: "openai/clip-vit-base-patch32"
  huggingface_hub_model_id: "helena-balabin/graph-clip"
  cache_dir: /data/huggingface/transformers/
  model_type: "image"
  graph_types: ["image", "action_image", "spatial_image"] # List of graph types to train models for
  graphormer_size: "small"
  dropout: 0.2
  alpha: 0.5

training:
  batch_size: 1024 # 32 for debug
  learning_rate: 1e-4
  max_steps: 1250 # 100 steps for debug
  weight_decay: 0.1
  logging_steps: 50
  eval_steps: 50  # 10 for debug
  save_steps: 250
  save_total_limit: 3
  warmup_ratio: 0.05
  warmup_ratio_unfreeze: 0.5
  precision: "bf16" # options: "bf16", "fp16", "fp32"
  pin_memory: true
  persistent_workers: true
  edge_max_dist: 20
  dataloader_drop_last: true

mlflow:
  tracking_uri: /data/mlflow/
  experiment_name: "GraphCLIP Multi-Graph Training"